{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nets import GenConv, DisConv, QConv\n",
    "from config import InfoGANConfig, SAVE_DIR\n",
    "from ops import mnist_for_gan, optimizer, clip, get_shape\n",
    "from utils import show_gray_image_3d\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.basicConfig(format = \"[%(asctime)s] %(message)s\", datefmt=\"%m%d %H:%M:%S\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_c(c_size, add_cv, index = -1):\n",
    "    '''\n",
    "    Args:\n",
    "        c_size - int\n",
    "            number of samples\n",
    "        add_v - int\n",
    "            number of additional continuous random variables [-1, 1] uniform\n",
    "        index - int\n",
    "            default to be -1\n",
    "    Return:\n",
    "        [c_size, 10 + add_cv]\n",
    "            10 for classification\n",
    "            add_cv for independent continuos\n",
    "    '''\n",
    "    \n",
    "    classify = np.zeros([c_size, 10])\n",
    "    conti = np.random.uniform(low = -1.0, high = 1.0, size = [c_size, add_cv])\n",
    "    if index < 0:\n",
    "        index = np.random.randint(10)\n",
    "    classify[:,index] = 1\n",
    "    return np.concatenate((classify, conti), axis = 1)\n",
    "\n",
    "def sample_z(z_size, z_dim):\n",
    "    return np.random.uniform(low=-1, high=1, size= [z_size, z_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class InfoGAN(InfoGANConfig):\n",
    "    def __init__(self):\n",
    "        InfoGANConfig.__init__(self)\n",
    "        logger.info(\"Building model starts...\")\n",
    "        tf.reset_default_graph()\n",
    "        self.generator = GenConv(name ='g_conv', batch_size=self.batch_size)\n",
    "        self.discriminator = DisConv(name='d_conv')\n",
    "        self.classifier = QConv(name='q_conv', c_dim=self.c_dim)\n",
    "        self.dataset = mnist_for_gan()\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape = [self.batch_size, self.x_size, self.x_size, self.x_channel])\n",
    "        self.Z = tf.placeholder(tf.float32, shape = [self.batch_size, self.z_dim])\n",
    "        self.C = tf.placeholder(tf.float32, shape = [self.batch_size, self.c_dim])\n",
    "        \n",
    "        self.G_sample = self.generator(tf.concat([self.Z, self.C], axis=1))\n",
    "        print(self.G_sample)\n",
    "        self.D_real = self.discriminator(self.X)\n",
    "        print(self.D_real)\n",
    "        self.D_fake = self.discriminator(self.G_sample, reuse = True)\n",
    "        print(self.D_fake)\n",
    "        self.Q_rct = self.classifier(self.G_sample)\n",
    "        print(self.Q_rct)\n",
    "        \n",
    "        self.Q_rct_classify, self.Q_rct_conti = tf.split(self.Q_rct, [10, self.c_dim-10],axis = 1)\n",
    "        self.C_classify, self.C_conti = tf.split(self.C, [10, self.c_dim-10], axis = 1)\n",
    "        \n",
    "        self.D_loss = -tf.reduce_mean(self.D_real)+tf.reduce_mean(self.D_fake)\n",
    "        self.Q_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.C_classify, logits=self.Q_rct_classify))+tf.reduce_mean(tf.square(self.C_conti-self.Q_rct_conti))\n",
    "        self.G_loss = tf.reduce_mean(self.D_fake)        \n",
    "\n",
    "        print(\"Generator_variables\")\n",
    "        self.generator.print_vars()\n",
    "        print(\"Discriminator_variables\")\n",
    "        self.discriminator.print_vars()\n",
    "        print(\"Classifier_variables\")\n",
    "        self.classifier.print_vars()\n",
    "\n",
    "        self.D_optimizer = optimizer(self.D_loss, self.discriminator.vars)\n",
    "        \n",
    "        with tf.control_dependencies([self.D_optimizer]):\n",
    "            self.D_optimizer_wrapped = [tf.assign(var, clip(var, -self.clip_b, self.clip_b)) for var in self.discriminator.vars]\n",
    "        \n",
    "        self.Q_optimizer = optimizer(self.Q_loss, self.generator.vars + self.classifier.vars)\n",
    "        self.G_optimizer = optimizer(self.G_loss, self.generator.vars)\n",
    "\n",
    "        logger.info(\"Building model done.\")\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize all variables in graph\"\"\"\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def restore(self):\n",
    "        \"\"\"Restore all variables in graph\"\"\"\n",
    "        logger.info(\"Restoring model starts...\")\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(SAVE_DIR))\n",
    "        logger.info(\"Restoring model done.\")     \n",
    "        \n",
    "    def train(self, train_epochs):\n",
    "        for epoch in range(train_epochs):\n",
    "            d_iter = 100 if epoch < 25 else 5\n",
    "            \n",
    "            for _ in range(d_iter):\n",
    "                X_sample = self.dataset(self.batch_size)\n",
    "                z_sample = sample_z(self.batch_size, self.z_dim)\n",
    "                c_sample = sample_c(self.batch_size, self.c_dim)\n",
    "                self.sess.run(self.D_optimizer_wrapped, feed_dict = {self.X : X_sample, self.Z : z_sample, self.C : c_sample})\n",
    "            \n",
    "            for _ in range(1):\n",
    "                self.sess.run(self.G_optimizer, feed_dict = {self.Z : z_sample, self.C : c_sample})\n",
    "            \n",
    "            for _ in range(1):\n",
    "                self.sess.run(self.Q_optimizer, feed_dict = {self.Z : z_sample, self.C : c_sample})\n",
    "                \n",
    "            if epoch % self.log_every == self.log_every+1:\n",
    "                X_sample = self.dataset(self.batch_size)\n",
    "                z_sample = sample_z(self.batch_size, self.z_dim)\n",
    "                c_sample = sample_c(self.batch_size, self.c_dim)\n",
    "                \n",
    "                D_loss = self.sess.run(self.D_loss, feed_dict = {self.X : X_sample, self.Z : z_sample, self.C : c_sample})\n",
    "                G_loss = self.sess.run(self.G_loss, feed_dict = {self.Z : z_sample, self.C : c_sample})\n",
    "                Q_loss = self.sess.run(self.Q_loss, feed_dict = {self.Z : z_sample, self.C : c_sample})\n",
    "                \n",
    "                gray_3d = sess.run(G_sample, feed_dict = {self.Z : z_sample, self.C : c_sample}) # self.batch_size x 28 x 28 x 1\n",
    "                gray_3d = np.squeeze(gray_3d)#self.batch_size x 28 x 28\n",
    "                show_gray_image_3d(gray_3d, col=5, fig_size = (10, 40), dataformat = 'CHW')\n",
    "                \n",
    "                logger.info(\"Epoch({}/{}) D_loss : {}, G_loss : {}, Q_loss : {}\".format(epoch+1, train_epochs, D_loss, G_loss, Q_loss))\n",
    "                saver.save(sess, os.path.join(SAVE_DIR, 'model'), global_step = epoch+1)\n",
    "                logger.info(\"Model save in %s\"%SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0722 11:48:47] Building model starts...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Tensor(\"g_conv/deconv2/Sigmoid:0\", shape=(100, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"d_conv/d_fc2/Sigmoid:0\", shape=(100, 1), dtype=float32)\n",
      "Tensor(\"d_conv_1/d_fc2/Sigmoid:0\", shape=(100, 1), dtype=float32)\n",
      "Tensor(\"q_conv/d_fc2/xw_plus_b:0\", shape=(100, 12), dtype=float32)\n",
      "Generator_variables\n",
      "g_conv/fc1/w:0:[112, 6272]\n",
      "g_conv/fc1/b:0:[6272]\n",
      "g_conv/deconv1/w:0:[5, 5, 64, 128]\n",
      "g_conv/deconv1/b:0:[64]\n",
      "g_conv/deconv2/w:0:[5, 5, 1, 64]\n",
      "g_conv/deconv2/b:0:[1]\n",
      "Discriminator_variables\n",
      "d_conv/conv1/w:0:[4, 4, 1, 32]\n",
      "d_conv/conv1/b:0:[32]\n",
      "d_conv/conv2/w:0:[4, 4, 32, 64]\n",
      "d_conv/conv2/b:0:[64]\n",
      "d_conv/d_fc1/w:0:[3136, 128]\n",
      "d_conv/d_fc1/b:0:[128]\n",
      "d_conv/d_fc2/w:0:[128, 1]\n",
      "d_conv/d_fc2/b:0:[1]\n",
      "Classifier_variables\n",
      "q_conv/conv1/w:0:[4, 4, 1, 32]\n",
      "q_conv/conv1/b:0:[32]\n",
      "q_conv/conv2/w:0:[4, 4, 32, 64]\n",
      "q_conv/conv2/b:0:[64]\n",
      "q_conv/d_fc1/w:0:[3136, 128]\n",
      "q_conv/d_fc1/b:0:[128]\n",
      "q_conv/d_fc2/w:0:[128, 12]\n",
      "q_conv/d_fc2/b:0:[12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0722 11:48:49] Building model done.\n"
     ]
    }
   ],
   "source": [
    "infogan = InfoGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.2]",
   "language": "python",
   "name": "conda-env-tf1.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
