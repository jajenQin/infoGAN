{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nets import GenConv, DisConv, QConv\n",
    "from config import InfoGANConfig, SAVE_DIR\n",
    "from ops import mnist_for_gan, optimizer, clip\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.basicConfig(format = \"[%(asctime)s] %(message)s\", datefmt=\"%m%d %H:%M:%S\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_c(c_size, add_cv, index = -1):\n",
    "    '''\n",
    "    Args:\n",
    "        c_size - int\n",
    "            number of samples\n",
    "        add_v - int\n",
    "            number of additional continuous random variables [-1, 1] uniform\n",
    "        index - int\n",
    "            default to be -1\n",
    "    Return:\n",
    "        [c_size, 10 + add_cv]\n",
    "            10 for classification\n",
    "            add_cv for independent continuos\n",
    "    '''\n",
    "    \n",
    "    classify = np.zeros([c_size, 10])\n",
    "    conti = np.random.uniform(low = -1.0, high = 1.0, size = [c_size, add_cv])\n",
    "    if index < 0:\n",
    "        index = np.random.randint(10)\n",
    "    classify[:,index] = 1\n",
    "    return np.concatenate((classify, conti), axis = 1)\n",
    "\n",
    "def sample_z(z_size, z_dim):\n",
    "    return np.random.uniform(low=-1, high=1, size= [z_size, z_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InfoGAN(infoGANConfig):\n",
    "    def __init__(self):\n",
    "        infoGANConfig.__init__(self)\n",
    "        self.generator = GenConv(name ='g_conv', batch_size=batch_size)\n",
    "        self.discriminator = DisConv(name='d_conv')\n",
    "        self.classifier = QConv(name='q_conv')\n",
    "        self.dataset = mnist_for_gan()\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape = [self.batch_size, self.x_size, self.x_size, self.x_size])\n",
    "        self.Z = tf.placeholder(tf.float32, shape = [self.batch_size, self.z_dim])\n",
    "        self.C = tf.placeholder(tf.float32, shape = [self.batch_size, self.c_dim])\n",
    "        \n",
    "        self.G_sample = self.generator(tf.concat([self.Z, self.C], axis=1))\n",
    "        self.D_real = self.discriminator(self.X)\n",
    "        self.D_fake = self.discriminator(self.G_sample, reuse = True)\n",
    "        self.Q_rct = self.classifier(self.G_sample, self.c_dim)\n",
    "        \n",
    "        self.Q_rct_classify, self.Q_rct_conti = tf.split(self.Q_rct, [10, self.c_dim-10],axis = 1)\n",
    "        \n",
    "        self.D_loss = -tf.reduce_mean(self.D_real)+tf.reduce_mean(self.D_fake)\n",
    "        self.Q_loss = tf.reduce_mena(self.)\n",
    "        self.G_loss = tf.reduce_mean(self.D_fake)        \n",
    "        \n",
    "        self.D_optimizer = optimizer(self.D_loss, self.discriminator.vars)\n",
    "        \n",
    "        with tf.control_dependencies([self.D_optimizer]):\n",
    "            self.D_optimizer_wrapped = [tf.assign(var, tf.clip_by_value(var, clip(var, -self.clip_b, self.clip_b))) for var in self.discriminator.vars]\n",
    "        \n",
    "        self.G_optimizer = optimizer(self.G_loss, self.generator.vars)\n",
    "        self.Q_optimizer = optimizer(self.Q_loss, self.generator.vars + self.classifier.vars)\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize all variables in graph\"\"\"\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def restore(self):\n",
    "        \"\"\"Restore all variables in graph\"\"\"\n",
    "        logger.info(\"Restoring model starts...\")\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(SAVE_DIR))\n",
    "        logger.info(\"Restoring model done.\")     \n",
    "        \n",
    "    def train(self, train_epochs):\n",
    "        for epoch in range(train_epochs):\n",
    "            d_iter = 100 if epoch < 25 else 5\n",
    "            \n",
    "            for _ in range(d_iter):\n",
    "                X_sample = self.dataset(self.batch_size)\n",
    "                z_sample = sample_z(self.batch_size, self.z_dim)\n",
    "                c_sample = sample_c(self.batch_size, self.c_dim)\n",
    "                self.sess.run(self.D_optimizer_wrapped, feed_dict = {self.X : X_sample, self.Z : z_sample, self.C : c_sample})\n",
    "            \n",
    "            for _ in range(1):\n",
    "                self.sess.run(self.G_optimizer, feed_dict = {self.Z : z_sample, self.C : c_sample})\n",
    "            \n",
    "            for _ in range(1):\n",
    "                self.sess.run(self.Q_optimizer, feed_dict = {self.Z : z_sample, self.C : c_sample})\n",
    "                \n",
    "            if epoch % self.log_every == self.log_every+1:\n",
    "                \n",
    "                saver.save(sess, os.path.join(SAVE_DIR, 'model'), global_step = epoch+1)\n",
    "                logger.info(\"Model save in %s\"%SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
